{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリのインポート:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas: データフレーム処理のためのライブラリ。\n",
    "- numpy: 数値演算のためのライブラリ。\n",
    "- matplotlib.pyplot: データの可視化のためのライブラリ。\n",
    "- train_test_split: データをトレーニングセットとテストセットに分割するための関数。\n",
    "- StandardScaler: データの標準化のためのクラス。\n",
    "- OneHotEncoder: カテゴリカル変数のワンホットエンコーディングのためのクラス。\n",
    "- ColumnTransformer: 異なる前処理ステップを複数の特徴量セットに適用するためのクラス。\n",
    "- Pipeline: データの前処理ステップを組み合わせるためのクラス。\n",
    "- Sequential, Dense, Dropout, BatchNormalization, LeakyReLU: Kerasのニューラルネ- ットワークモデルを構築するためのクラス。\n",
    "- to_categorical: クラスラベルをバイナリ行列に変換するためのユーティリティ関数。\n",
    "- Adam: 最適化アルゴリズム Adam を実装したクラス。\n",
    "- EarlyStopping, LearningRateScheduler: コールバック関数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./we-are-the-best/yabindra-data/newdata/train.csv')\n",
    "test = pd.read_csv('./we-are-the-best/yabindra-data/newdata/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの事前処理\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- トレーニングデータとテストデータの CSV ファイルを読み込む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('ID', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'ID' 列を削除して、余分な情報を取り除く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値のランダムな値で埋める関数\n",
    "def fill_na_random(df):\n",
    "    columns_to_fill = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "    for column in columns_to_fill:\n",
    "        non_nan_values = df[column].dropna()\n",
    "        nan_count = df[column].isna().sum()\n",
    "        random_values = np.random.choice(non_nan_values, nan_count)\n",
    "        df[column][df[column].isna()] = random_values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データに対して欠損値のランダムな値で埋める\n",
    "train = fill_na_random(train)\n",
    "\n",
    "# テストデータに対して欠損値のランダムな値で埋める\n",
    "test = fill_na_random(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 損値をランダムな値で埋める関数。トレーニングデータとテストデータのそれぞれに対して適用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値の処理\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もしもデータが入っていなかったら 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_engineering(data):\n",
    "    data['Income_per_Alien'] = data['Income'] / (data['Alien'].replace(0, 1))  # Avoid division by zero\n",
    "    data['Income_per_Alien'] = data['Income_per_Alien'].astype(int)\n",
    "\n",
    "    data['TotalPower'] = data['SpacePort'] + data['PowerPlant'] + + data['Meteor']\n",
    "    data['HighIncome'] = (data['Income'] > data['Income'].median()).astype(int)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = feature_engineering(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income_per_Alien:\n",
    "\n",
    "Alienごとの平均収入を計算します。Alienが存在しない場合（Alienが0の場合）、除算を避けるために1で置き換えます。\n",
    "astype(int) により、小数点以下を切り捨てて整数型に変換されます。\n",
    "\n",
    "\n",
    "## TotalPower (合計パワー):\n",
    "\n",
    "'SpacePort' と 'PowerPlant' の合計パワーを計算します。\n",
    "\n",
    "## HighIncome (高収入フラグ):\n",
    "\n",
    "'Income' 列の中央値よりも高い収入の場合、1（True）に、それ以外の場合、0（False）に変換します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = train.columns.tolist()\n",
    "print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "train_data, validation_data = train_test_split(train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train` データセットをトレーニングデータと検証データに分割します。`test_size=0.2` は、検証データの割合を示しており、`random_state=42` はランダムな分割の再現性を担保しています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量の定義\n",
    "features = ['Medical', 'PowerPlant', 'Natural', 'SpacePort', 'Alien', 'Income', 'Education', 'PopulationDensity', 'Murder', 'Module', 'Purpose']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カテゴリカル特徴量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Module', 'Purpose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを前処理するための関数\n",
    "def preprocess_data(data, features, target_column='value'):\n",
    "    if target_column in data.columns:\n",
    "        X = data[features]\n",
    "        y = data[target_column]\n",
    "\n",
    "        # 数値とカテゴリカル特徴量の前処理器を定義\n",
    "        numeric_features = X.columns.difference(categorical_features)\n",
    "        \n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "        # ColumnTransformerを使用して前処理器を組み合わせる\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "\n",
    "         # データを前処理\n",
    "        X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "       # ラベルをカテゴリカルに変換\n",
    "        label_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'S': 4}\n",
    "        y_numeric = y.map(label_mapping)\n",
    "        y_binary = to_categorical(y_numeric, num_classes=5)\n",
    "\n",
    "        return X_preprocessed, y_binary\n",
    "    else:\n",
    "        X = data[features]\n",
    "\n",
    "        # 数値とカテゴリカル特徴量の前処理器を定義\n",
    "        numeric_features = X.columns.difference(categorical_features)\n",
    "        \n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "        # ColumnTransformerを使用して前処理器を組み合わせる\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "\n",
    "       # データを前処理\n",
    "        X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "        return X_preprocessed, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#関数の呼び出し\n",
    "X_train_scaled, y_train_binary = preprocess_data(train_data, features)\n",
    "X_validation_scaled, y_validation_binary = preprocess_data(validation_data, features)\n",
    "X_test_scaled, _ = preprocess_data(test, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(512, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Hidden layers\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- この関数では、Sequential モデルを作成し、Dense レイヤーを使用して畳み込みニューラルネットワーク（CNN）を構築しています。\n",
    "- ニューラルネットワークには、入力層、隠れ層、および出力層が含まれています。\n",
    "- 各層では、活性化関数、正規化、およびドロップアウトが適用されています。\n",
    "- 最後に、モデルをコンパイルして、最適化アルゴリズム、損失関数、および評価メトリクスを指定しています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduling\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lr * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- この関数は、学習率をスケジューリングするためのもので、ニューラルネットワークの学習中に動的に学習率を変更します。\n",
    "- initial_lr は初期学習率を示しており、ここでは 0.001 と設定されています。\n",
    "- drop は学習率をどれだけ減衰させるかを示しており、ここでは 0.5 と設定されています。\n",
    "- epochs_drop は学習率を減衰させるエポックの間隔を示しています。10 エポックごとに学習率が減衰します。\n",
    "- np.floor((1+epoch)/epochs_drop) は現在のエポック数を epochs_drop で割って小数点以下を切り捨てています。\n",
    "- 学習率 lrate は、初期学習率を減衰率に基づいて計算され、各エポックごとに変化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(step_decay)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lr_scheduler は、学習率のスケジュールを設定するための LearningRateScheduler コールバックです。このコールバックは、各エポックの最初に step_decay 関数を呼び出し、その結果を学習率に適用します。これにより、学習率が動的に変化し、最適な学習率スケジュールを実現できます。\n",
    "\n",
    "- early_stopping は、訓練が収束したかどうかを監視し、早期に訓練を停止するためのコールバックです。monitor='val_loss' は検証データの損失を監視し、patience=10 は改善が見られない場合に、10エポック以上損失が改善されない場合に訓練を停止します。restore_best_weights=True は、最良の重みで訓練が停止した場合に、その時点のモデルの重みを復元する設定です。これにより、最も性能が良いモデルが保存されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = build_nn_model(X_train_scaled.shape[1])\n",
    "model = nn_model.fit(X_train_scaled, y_train_binary, epochs=50, batch_size=64, validation_data=(X_validation_scaled, y_validation_binary), callbacks=[lr_scheduler, early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nn_model = build_nn_model(X_train_scaled.shape[1]): ニューラルネットワークモデルを構築します。X_train_scaled.shape[1] は入力の次元数で、これはニューラルネットワークの最初の層の入力次元数として使用されます。\n",
    "\n",
    "- nn_model.fit(): ニューラルネットワークを実際に訓練します。以下は各引数の説明です：\n",
    "\n",
    "- X_train_scaled, y_train_binary: 訓練データの入力 (X_train_scaled) と対応するラベル (y_train_binary)。\n",
    "- epochs=50: エポック数は 50 で、データ全体を 50 回処理します。\n",
    "- batch_size=32: ミニバッチ学習を行い、各バッチのサイズは 32 です。\n",
    "- validation_data=(X_validation_scaled, y_validation_binary): 検証データを指定し、各エポック終了後にモデルの性能を検証します。\n",
    "- callbacks=[lr_scheduler, early_stopping]: コールバック関数として、学習率のスケジュール (lr_scheduler) と早期停止 (early_stopping) を使用します。これにより、動的な学習率調整と早期停止が実現されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータに対する予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 損失値と精度の推移を取得\n",
    "loss = model.history['loss']\n",
    "accuracy = model.history['accuracy']\n",
    "\n",
    "# グラフの描画\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, accuracy, 'r', label='Training accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに対して予測を行う\n",
    "y_test_probs = nn_model.predict(X_test_scaled)\n",
    "y_test = np.argmax(y_test_probs, axis=1)\n",
    "\n",
    "# 'ID' 列と予測値を含む DataFrame を作成する\n",
    "submission_df = pd.DataFrame({'ID': test['ID'], 'value': y_test})\n",
    "\n",
    "# 'value' 列の NaN 値を処理する\n",
    "submission_df['value'] = submission_df['value'].map({0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'S'}).fillna('A')\n",
    "\n",
    "# DataFrame を CSV ファイルに保存する\n",
    "submission_df.to_csv('categorical.csv', index=False)\n",
    "print(\"Submission saved to submission_nn_with_categorical.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
